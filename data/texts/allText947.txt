What exactly is this project about?It is about using cloud computing and machine learning to classify a huge portion of existing websites into a set of categories. The resulting URL to category mappings are made available as an online API and also as a downloadable list so you can integrate and use it with other software and services.What would such a list be used for?What you need the categories for can vary. Perhaps you’re building an Internet website filter and need to block certain categories such as adult and social networking sites. Maybe you are running a hosting company and want to enforce an Acceptable Use Policy to prevent customers from running a file sharing or classified ads network. These are a few examples but really it can be integrated into any product or service that requires a better representation of a site and its content.How does it work?There are two ways you can access the categories: a downloadable website to category database and an online accessible API. The downloadable list might not always be the most practical, especially if you are on a device with limited storage space. With the API there is no need to download the list, you simply make a HTTP GET request to the API and get back the categories as a JSON response. Sample API JSON Result The API has some additional benefits too. For instance, if we don’t have the URL or site categorized yet we will crawl it and categorize it for you. If you have a need to know a category, and don’t have the luxury of waiting for the next downloadable list, you would have the answer in a short amount of time.Another benefit to using the API is that it is constantly updated with the latest sites. As we classify new sites they get added to the database as close to real time as possible so that future requests for that site return the right category.Category lookups can be done on full URLs, IP addresses, or just a hostname. When you request a hostname or IP address, we'll return a list of all the categories for the content we've seen published on the site. What’s the current status?We have a fully functioning “crawler robot” that is able to return a category for a site. In fact, we’ve been running a crawl to build the first list throughout the month of August.The preview of the API is now available!To use it just make a GET request in the following format:https://categories.webshrinker.com/preview/lookup/<base64 value of the URL>Sample GET requests: http://www.msn.com: https://categories.webshrinker.com/preview/lookup/aHR0cDovL3d3dy5tc24uY29thttp://www.yahoo.com: https://categories.webshrinker.com/preview/lookup/aHR0cDovL3d3dy55YWhvby5jb20=https://www.facebook.com: https://categories.webshrinker.com/preview/lookup/aHR0cHM6Ly93d3cuZmFjZWJvb2suY29tA demo PHP script can be found here:https://gist.github.com/aspotton/640778079913af8c0accOther things to keep in mind: If the URL you requested is uncategorized check again in about 10-15 seconds and there should be a category for it in the latest response You can try a different protocol (http vs https) We don't assume http and https are the same until they are crawled You can try categorizing just the hostname like www.msn.com or www.yahoo.com for a broader categorization Ok, so why crowdfunding?So far we have classified about a quarter of a billion sites - but the Internet is big and we need your help bring this home! Now that we have the backend in place we need to actually categorize as many sites as we can to make the list more useful. The majority of the funds raised will be used to launch a large cluster of computers on AWS (Amazon Web Services) to build the list much more quickly.We also rely on a bunch of open source projects and would like to give back. By backing us you are not only helping to get this off the ground, you are also helping to return good karma to some pretty wonderful open source projects that can make use of this technology. We think by allowing the use of the list by home and non-profit organizations will help to accomplish that.Also, we’d like to support filtering projects like SquidGuard / Dans Guardian which offers people a way to filter Internet sites without paying crazy fees to do so. The only problem we’ve seen with projects like these are the quality of the lists they use to actually do the filtering. We’d like to reach out to the maintainers of projects such as these when we have a quality list so everyone can benefit. Other open source projects would also be considered so if you know of others please comment and let us know!What would the funds be used for?The majority of the funds raised will go to Amazon Web Services to categorize as many URLs, sites, and IP addresses as possible. To do this we need to run a large number of virtual machines for extended periods of time.There are some other areas that we'd like to improve upon as well, including: Deciding on a final, universal format for the downloadable list Improving category classification for non-English websites Adding additional categories like Proxy and Filter Avoidance and Analytics and Trackers (update: "Proxy and Filter Avoidance" has been added!) What are the current categories used by the list?This is the first version of the category list and suggestions are welcome: Abortion Adult Content Advertising Alcohol & Tobacco Blogs & Personal Sites Business Chat & Instant Messaging Dating & Personals Drugs Economy & Finance Education & Self-Help Entertainment Food & Recipes Gambling Games Hacking & Cracking Health Information Technology Jobs & Careers Media Sharing Message Boards & Forums News & Media Proxy and Filter Avoidance Real Estate Religion Search Engines & Portals Shopping Social Networking Streaming Media Travel Uncategorized Vehicles Virtual Reality Weapons Risks and challenges We already have infrastructure in place for dealing with large volumes of information and the API by leveraging Amazon Web Services. We're using it in a way so that the service can easily scale and handle the requests. The component that still needs a bit of work is the format for the downloadable category list. Building a downloadable list of all possible sites uses a lot of space, needs to be fast, and be as compatible as possible. We're even thinking of offering a few tiers of category lists, one that contains more highly accessed sites and an "everything" list. Putting the format for these lists together may take a bit of time. Besides that, right now there aren't too many risks involved with launching as the infrastructure is in place. We just need to compile as big of a list as possible so every bit helps. Learn about accountability on Kickstarter